#!/usr/bin/env python3
"""Step 4 è¡¥è·‘: å¯¹ ASR è½¬å†™çš„è§†é¢‘åšåˆ†ç±»ã€‚

å¤ç”¨ step4 çš„åˆ†ç±»é€»è¾‘ï¼Œè¾“å…¥æ”¹ä¸º transcripts_asr ç›®å½•ã€‚
ç»“æœåˆå¹¶åˆ° classification.jsonã€‚
"""

import json
import os
import sys
import time
import urllib.request
import urllib.error

BASE_DIR = "/root/projects/bili-transcripts"
VIDEOS_JSON = f"{BASE_DIR}/data/raw/videos.json"
TRANSCRIPTS_ASR_DIR = f"{BASE_DIR}/data/transcripts_asr"
OUTPUT_DIR = f"{BASE_DIR}/data/classified"
OUTPUT_FILE = f"{OUTPUT_DIR}/classification.json"
PROGRESS_FILE = f"{OUTPUT_DIR}/.progress_asr.json"

import json as _json_creds
with open(os.path.join(os.path.dirname(os.path.abspath(__file__)), "../config/credentials.json")) as _f:
    _creds = _json_creds.load(_f)
API_URL = _creds.get("classify", {}).get("api_url", "https://api.openai.com/v1/chat/completions")
API_KEY = _creds.get("classify", {}).get("api_key", "")
MODEL = "gemini-3-flash-preview"

CATEGORY_SCHEMA = """åˆ†ç±»ä½“ç³»ï¼ˆä¸»åˆ†ç±» â†’ å­åˆ†ç±»ï¼‰ï¼š

ğŸ­ äººæƒ…ä¸–æ•…
  - ç¤¾äº¤æ²Ÿé€šï¼ˆå¤¸äººã€æ¥è¯ã€èŠå¤©æŠ€å·§ã€small talkã€é“æ­‰ï¼‰
  - é¥­å±€é…’å±€ï¼ˆæ•¬é…’ã€é¥­å±€èŠå¤©ã€é€ç¤¼ã€å®´è¯·ï¼‰
  - èŒåœºäººé™…ï¼ˆè¾¹ç•Œæ„Ÿã€éº»çƒ¦åˆ«äººã€é¢†å¯¼ç›¸å¤„ï¼‰
  - æ‹çˆ±å…³ç³»ï¼ˆæš§æ˜§è¾¹ç•Œã€æ‹çˆ±æ¨¡å¼ã€åˆ†æ‰‹ã€çº¦ä¼šï¼‰

ğŸ’¼ èŒä¸šå‘å±•
  - æ±‚èŒé¢è¯•ï¼ˆç®€å†ã€é¢è¯•æŠ€å·§ã€æ‰¾å·¥ä½œï¼‰
  - åˆ›ä¸šå•†ä¸šï¼ˆå•†ä¸šæ€ç»´ã€åˆ›ä¸šå¤ç›˜ã€å˜ç°ã€å¤–è´¸ï¼‰
  - èŒåœºæˆé•¿ï¼ˆåˆå…¥èŒåœºã€è·³æ§½ã€èŒä¸šè§„åˆ’ï¼‰

ğŸ§  è®¤çŸ¥æˆé•¿
  - æ€ç»´æ–¹æ³•ï¼ˆå­¦ä¹ æ–¹æ³•ã€æ€ç»´æ¨¡å‹ã€è®¡åˆ’åˆ¶å®šï¼‰
  - å¿ƒç†è‡ªæˆ‘ï¼ˆå†…è€—ã€è‡ªæˆ‘è®¤çŸ¥ã€æƒ…ç»ªç®¡ç†ï¼‰
  - å“²å­¦æ€è¾¨ï¼ˆå“²å­¦ã€ç¤¾ä¼šæ‰¹åˆ¤ã€æ·±åº¦æ€è€ƒï¼‰

ğŸ’» æŠ€æœ¯å·¥å…·
  - ç¼–ç¨‹å¼€å‘ï¼ˆç¼–ç¨‹è¯­è¨€ã€æ¡†æ¶ã€ç³»ç»Ÿã€ç®—æ³•ï¼‰
  - AI åº”ç”¨ï¼ˆAIç¼–ç¨‹ã€AIè§†é¢‘ã€AIå·¥å…·ã€LLMï¼‰
  - æ•ˆç‡å·¥å…·ï¼ˆç¬”è®°è½¯ä»¶ã€Macé…ç½®ã€æµè§ˆå™¨ã€çª—å£ç®¡ç†ï¼‰
  - è®¾è®¡åˆ›ä½œï¼ˆPS/PPT/PR/AE/UIè®¾è®¡ã€è§†é¢‘å‰ªè¾‘ï¼‰

ğŸ“š å­¦ä¸šè€ƒè¯•
  - å…¬è€ƒï¼ˆç”³è®ºã€è¡Œæµ‹ã€é¢è¯•ï¼‰
  - å­¦æœ¯ç§‘ç ”ï¼ˆè®ºæ–‡å†™ä½œã€æ•°å­¦å»ºæ¨¡ã€LaTeXï¼‰
  - å¤§å­¦è¯¾ç¨‹ï¼ˆæ•°å­¦ã€è®¡ç®—æœºã€è‹±è¯­ï¼‰

ğŸ¬ å½±è§†å¨±ä¹
  - å½±è§†è§£è¯´ï¼ˆç”µå½±/å‰§é›†è§£è¯´ã€åŠ¨ç”»ï¼‰
  - ç»¼è‰ºè„±å£ç§€ï¼ˆå°å“ã€è„±å£ç§€ã€ç»¼è‰ºï¼‰
  - éŸ³ä¹ASMRï¼ˆéŸ³ä¹ã€ASMRã€æ²»æ„ˆï¼‰

ğŸ³ ç”Ÿæ´»æ–¹å¼
  - ç¾é£Ÿçƒ¹é¥ªï¼ˆèœè°±ã€ç¾é£Ÿæ¢åº—ï¼‰
  - æ•°ç å¥½ç‰©ï¼ˆæ•°ç äº§å“ã€å¥½ç‰©æ¨èï¼‰
  - ç”Ÿæ´»æŠ€å·§ï¼ˆè£…ä¿®ã€æ”¶çº³ã€æ—¥å¸¸æŠ€å·§ï¼‰
  - æ‘„å½±æ‹ç…§ï¼ˆæ‹ç…§æŠ€å·§ã€æ‘„å½±æ•™ç¨‹ï¼‰

ğŸŒ æ·±åº¦å†…å®¹
  - äººç‰©è®¿è°ˆï¼ˆæ’­å®¢ã€æ·±åº¦å¯¹è¯ã€äººç‰©æ•…äº‹ï¼‰
  - è¡Œä¸šæ´å¯Ÿï¼ˆè¡Œä¸šåˆ†æã€å•†ä¸šæ¡ˆä¾‹ã€ç¤¾ä¼šè§‚å¯Ÿï¼‰
  - æŠ•èµ„ç†è´¢ï¼ˆæŠ•èµ„ã€ç†è´¢ã€è´¢åŠ¡è‡ªç”±ï¼‰"""

SYSTEM_PROMPT = f"""ä½ æ˜¯ä¸€ä¸ªè§†é¢‘å†…å®¹åˆ†ç±»ä¸“å®¶ã€‚æ ¹æ®è§†é¢‘çš„æ ‡é¢˜ã€ç®€ä»‹å’Œæ–‡ç¨¿å†…å®¹ï¼Œå¯¹è§†é¢‘è¿›è¡Œåˆ†ç±»ã€‚

{CATEGORY_SCHEMA}

è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼Œä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ï¼š
{{"primary_category": "ä¸»åˆ†ç±»åï¼ˆå¦‚ï¼šäººæƒ…ä¸–æ•…ï¼‰", "sub_category": "å­åˆ†ç±»åï¼ˆå¦‚ï¼šç¤¾äº¤æ²Ÿé€šï¼‰", "confidence": 0.95, "summary": "ä¸€å¥è¯æ¦‚æ‹¬è§†é¢‘æ ¸å¿ƒå†…å®¹ï¼ˆ20-40å­—ï¼‰", "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2", "æ ‡ç­¾3"]}}

è§„åˆ™ï¼š
1. confidence èŒƒå›´ 0.0-1.0ï¼Œè¡¨ç¤ºåˆ†ç±»ç½®ä¿¡åº¦
2. å¦‚æœè§†é¢‘æ˜æ˜¾è·¨åˆ†ç±»ï¼Œé€‰æœ€ä¸»è¦çš„ï¼Œconfidence é€‚å½“é™ä½
3. tags ç»™ 2-5 ä¸ªå…³é”®è¯æ ‡ç­¾
4. summary åŸºäºæ–‡ç¨¿å†…å®¹å†™ï¼Œä¸è¦åªçœ‹æ ‡é¢˜çŒœ"""


def api_call(messages, retries=3):
    payload = json.dumps({
        "model": MODEL,
        "messages": messages,
        "temperature": 0.1,
        "max_tokens": 500,
    }).encode("utf-8")

    req = urllib.request.Request(
        API_URL,
        data=payload,
        headers={
            "Authorization": f"Bearer {API_KEY}",
            "Content-Type": "application/json",
        },
    )

    for attempt in range(retries):
        try:
            with urllib.request.urlopen(req, timeout=30) as resp:
                data = json.loads(resp.read().decode("utf-8"))
            content = data["choices"][0]["message"]["content"].strip()
            if content.startswith("```"):
                content = content.split("\n", 1)[1]
                content = content.rsplit("```", 1)[0]
            return json.loads(content)
        except Exception as e:
            print(f"  attempt {attempt+1} failed: {e}")
            if attempt < retries - 1:
                time.sleep(2 * (attempt + 1))
    return None


def main():
    with open(VIDEOS_JSON) as f:
        all_videos = {v["bvid"]: v for v in json.load(f)["videos"]}

    # Load existing classification
    existing_classified = {}
    if os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE) as f:
            data = json.load(f)
            existing_classified = {v["bvid"]: v for v in data["videos"]}
    print(f"Existing classified: {len(existing_classified)}")

    # Find ASR transcripts not yet classified
    asr_bvids = sorted(
        f.replace(".txt", "")
        for f in os.listdir(TRANSCRIPTS_ASR_DIR)
        if f.endswith(".txt")
    )
    already_classified = {v["bvid"] for v in existing_classified.values()}

    os.makedirs(OUTPUT_DIR, exist_ok=True)
    progress = {}
    if os.path.exists(PROGRESS_FILE):
        with open(PROGRESS_FILE) as f:
            progress = json.load(f)

    remaining = [b for b in asr_bvids if b not in already_classified and b not in progress]
    total = len(asr_bvids)
    done = len(progress)
    print(f"ASR total: {total}, Already done: {len(already_classified & set(asr_bvids))}, Progress: {done}, Remaining: {len(remaining)}")

    if not remaining:
        print("All ASR classified!")
        merge_output(all_videos, existing_classified, progress)
        return

    errors = 0
    for i, bvid in enumerate(remaining):
        video = all_videos.get(bvid)
        if not video:
            print(f"[{done+i+1}/{total}] {bvid} â€” no metadata, skip")
            continue

        tpath = os.path.join(TRANSCRIPTS_ASR_DIR, f"{bvid}.txt")
        with open(tpath, encoding="utf-8") as f:
            transcript = f.read()[:1500]

        upper = video["upper"]["name"] if isinstance(video["upper"], dict) else video["upper"]
        desc = (video.get("desc", "") or "")[:200]
        dur_min = video["duration"] / 60

        user_msg = f"è§†é¢‘ä¿¡æ¯ï¼š\næ ‡é¢˜ï¼š{video['title']}\nUPä¸»ï¼š{upper}\næ—¶é•¿ï¼š{dur_min:.0f}åˆ†é’Ÿ\nç®€ä»‹ï¼š{desc}\n\næ–‡ç¨¿å‰1500å­—ï¼š\n{transcript}"

        print(f"[{done+i+1}/{total}] {bvid} â€” {video['title'][:40]}...", end=" ", flush=True)

        result = api_call([
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_msg},
        ])

        if result:
            progress[bvid] = result
            print(f"â†’ {result['primary_category']}/{result['sub_category']} ({result['confidence']})")
        else:
            progress[bvid] = {"error": "failed"}
            errors += 1
            print("â†’ FAILED")

        if (i + 1) % 10 == 0:
            with open(PROGRESS_FILE, "w") as f:
                json.dump(progress, f, ensure_ascii=False, indent=2)
            print(f"  [saved {done+i+1}/{total}]")

        time.sleep(0.5)

    with open(PROGRESS_FILE, "w") as f:
        json.dump(progress, f, ensure_ascii=False, indent=2)

    merge_output(all_videos, existing_classified, progress)
    print(f"\nDone! Errors: {errors}")


def merge_output(all_videos, existing_classified, new_progress):
    """Merge existing classification with new ASR classifications."""
    results = list(existing_classified.values())

    for bvid, cls in new_progress.items():
        if "error" in cls:
            continue
        v = all_videos.get(bvid, {})
        upper = v.get("upper", {})
        results.append({
            "bvid": bvid,
            "title": v.get("title", ""),
            "upper": upper.get("name", "") if isinstance(upper, dict) else upper,
            "duration": v.get("duration", 0),
            "link": f"https://www.bilibili.com/video/{bvid}",
            "cover": v.get("cover", ""),
            "pubdate": v.get("pubdate", ""),
            "classification": cls,
            "source": "asr",
        })

    results.sort(key=lambda x: (x["classification"]["primary_category"], x["classification"]["sub_category"]))

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump({
            "meta": {
                "total": len(results),
                "classified_at": time.strftime("%Y-%m-%dT%H:%M:%S"),
                "model": MODEL,
                "note": "merged: subtitle + asr"
            },
            "videos": results
        }, f, ensure_ascii=False, indent=2)
    print(f"Output: {OUTPUT_FILE} ({len(results)} videos)")


if __name__ == "__main__":
    main()
